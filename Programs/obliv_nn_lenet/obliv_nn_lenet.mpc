from Compiler import matrix_lib
from Compiler import folding_lib
from Compiler import mpc_math
from Compiler import relu_lib
from Compiler import relu_parallel_lib

#### NAMING CONVENTIONS FOR THIS FILE ####
# A Matrix in memory is denoted with capital letter: A
# A Matrix in a registry vector is denoted with low case and suffix "_f" (flattened): a_f
# A transformed matrix uses the suffix "tr": Atr or atr_f
# A multiplication of 2 matrixes is dentoted with "_" between them: A_B or a_b_f
# A matrix in memory uses the type as suffix when is used with different types: A_sint / A_sfix
# A transposed matrix uses the suffix "tp": Atp
# A pruned matrix (some rows removed by the stride) attaches suffix pr: Apr (normally applied to Atr, so Atrpr)
# The number of rows/columns of a matrix is denoted with prefix "rows_ / cols_": cols_A
# The array of rows of columns of a matrix is denoted with suffix "_rows / _cols": A_cols
# Single-use matrixes (without algorithmic meaning) are denoted with prefix "_" or "__" (Erlang-like): _A or _a_f
# Note that, the suffix "_" does not have any special meaning and can be used arbitrarily
# sometimes a single row/column is allocated in registry memory, this is denoted as: a_row_f / a_col_f


# X is a 3D feature matrix
# Y is an array of 3D kernels - 1 item per layer
# T is an array of of tuples with conv triples - 1 item per layer

def oblivious_lenet(X, Y, Biases, Triple):

#######   LAYER 1   #######

    h = 32
    w = 32
    kh = 5
    kw = 5
    s = 1
    s_ = 6

    X1_layer = matrix_lib.rearrange_3d_features_into_2d_matrix(X)
    K1_layer = matrix_lib.rearrange_4d_kernels_into_2d_matrix(Y[0])
    X1_convolved = matrix_lib.conv3d_sfix2sfix(X1_layer, K1_layer, Triple[0][0], Triple[0][1], Triple[0][2], kh, kw, s, s_, h, w)
    X1_biased = matrix_lib.add_matrices(X1_convolved, Biases[0])
    X1_activated = relu_parallel_lib.relu_2d_parallel(X1_biased)

    h = 28
    w = 28
    kh = 2
    kw = 2
    stride = 2

    X1_folded = folding_lib.folding(X1_activated, kh, kw, stride, "avg_pool", h, w)


#######   LAYER 2   #######

    X2_layer = X1_folded

    h = 14
    w = 14
    kh = 5
    kw = 5
    s = 6
    s_ = 16

    K2_layer = matrix_lib.rearrange_4d_kernels_into_2d_matrix(Y[1])
    X2_convolved = matrix_lib.conv3d_sfix2sfix(X2_layer, K2_layer, Triple[1][0], Triple[1][1], Triple[1][2], kh, kw, s, s_, h, w)
    X2_biased = matrix_lib.add_matrices(X2_convolved, Biases[1])
    X2_activated = relu_parallel_lib.relu_2d_parallel(X2_biased)

    h = 10
    w = 10
    kh = 2
    kw = 2
    stride = 2
    X2_folded = folding_lib.folding(X2_activated, kh, kw, stride, "avg_pool", h, w)


#######   LAYER 3   #######

    X3_layer = X2_folded

    h = 5
    w = 5
    kh = 5
    kw = 5
    s = 16
    s_ = 120

    K3_layer = matrix_lib.rearrange_4d_kernels_into_2d_matrix(Y[2])
    X3_convolved = matrix_lib.conv3d_sfix2sfix(X3_layer, K3_layer, Triple[2][0], Triple[2][1], Triple[2][2], kh, kw, s, s_, h, w)
    X3_biased = matrix_lib.add_matrices(X3_convolved, Biases[2])
    X3_activated = relu_parallel_lib.relu_2d_parallel(X3_biased)

#######   LAYER 4   #######

    X4_layer = X3_activated

    # s = 120
    # s_ = 84
    # (wh x s) where wh=1 and s=120

    FC1_layer = Y[3]
    X4_fullyconnected = matrix_lib.multmat_sfix2sfix(X4_layer, FC1_layer, Triple[3][0], Triple[3][1], Triple[3][2])
    X4_biased = matrix_lib.add_matrices(X4_fullyconnected, Biases[3])
    X4_activated = relu_parallel_lib.relu_2d_parallel(X4_biased)

#######   LAYER 5   #######

    X5_layer = X4_activated

    # s = 84
    # s_ = 10
    # (wh x s) where wh=1 and s=84

    FC2_layer = Y[4]
    X5_fullyconnected = matrix_lib.multmat_sfix2sfix(X5_layer, FC2_layer, Triple[4][0], Triple[4][1], Triple[4][2])
    X5_biased = matrix_lib.add_matrices(X5_fullyconnected, Biases[4])
    X5_final = matrix_lib.flatten_to_array_reg(X5_biased)
    X5_softmaxed = folding_lib.softmax_scaled(X5_final, -15, True)


    results = [X5_softmaxed, X5_final, X4_activated, X3_activated, X2_folded, X1_folded, X4_fullyconnected, X4_biased]

    return results
